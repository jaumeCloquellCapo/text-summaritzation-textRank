---
title: "textRank"
author: "jaume cloquell capo"
date: "21 de mayo de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

https://www.hvitfeldt.me/blog/tidy-text-summarization-using-textrank/#data-preparation

We start by loading the appropriate packages, which include tidyverse for general tasks, tidytext for text manipulations, textrank for the implementation of the TextRank algorithm and finally rvest to scrape an article to use as an example. The github for the textrank package can be found here.

```{r cars}
library(tidyverse)
library(tidytext)
library(textrank)
library(rvest)
library(tm)
```

To showcase this method I have randomly (EXTENSIVELY filtered political and controversial) selected an article as our guinea pig. The main body is selected using the html_nodes.

```{r pressure, echo=FALSE}
url <- "https://www.elmundo.es/espana/2019/05/21/5ce3fa30fdddff7b688b45fa.html"
article <- read_html(url) %>%
  html_nodes('div[class="ue-l-article__body ue-c-article__body"]') %>%
  html_text()
```

next we load the article into a tibble (since tidytext required the input as a data.frame). We start by tokenize according to sentences which is done by setting token = "sentences" in unnest_tokens. The tokenization is not always perfect using this tokenizer, but it have a low number of dependencies and is sufficient for this showcase. Lastly we add sentence number column and switch the order of the columns (textrank_sentences prefers the columns in a certain order).

```{r}
article_sentences <- tibble(text = article) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)
```

next we will tokenize again but this time to get words. In doing this we will retain the sentence_id column in our data.

```{r}
article_words <- article_sentences %>%
  unnest_tokens(word, sentence)
article_words
```
```{r}
article_words %>%
  count(word, sort = TRUE) 
```


```{r}

library(ggplot2)

article_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r}
spanish_stop_words <- bind_rows(stop_words,
                               data_frame(word = tm::stopwords("es"),
                                          lexicon = "custom"))
spanish_stop_words
```


```{r}
stopwords(kind = "es")
article_words <- article_words %>%
  anti_join(stop_words, by = "word")
article_words
```

```{r}
article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)
article_summary
```

