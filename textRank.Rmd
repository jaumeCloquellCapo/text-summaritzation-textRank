---
title: "textRank"
author: "jaume cloquell capo"
date: "21 de mayo de 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text Rank

Comenzamos cargando los paquetes apropiados, que incluyen tidyverse para tareas generales, tidyverse para manipulaciones de texto, textrank para la implementación del algoritmo TextRank y finalmente rvest para raspar un artículo para usarlo como ejemplo. El github para el paquete textrank se puede encontrar https://github.com/bnosac/textrank

```{r cars}
library(tidyverse)
library(tidytext)
library(textrank)
library(rvest)
library(tm)
library(lexRankr)
```

Para mostrar este método he seleccionado al azar  un artículo de nuestro diario nacional "elmundo" El cuerpo principal se selecciona utilizando los html_nodes.

```{r pressure, echo=FALSE}
url <- "https://www.elmundo.es/espana/2019/05/21/5ce3fa30fdddff7b688b45fa.html"
article <- read_html(url) %>%
  html_nodes('div[class="ue-l-article__body ue-c-article__body"]') %>%
  html_text()
```

a continuación cargamos el artículo en un tibble (ya que tidytext requería la entrada como data.frame). Comenzamos por tokenize según las frases, lo que se hace estableciendo token = "sentences" en unnest_tokens. La tokenización no siempre es perfecta con este tokenizador, pero tiene un número bajo de dependencias y es suficiente para este trabajo. Por último añadimos la columna de número de frase y cambiamos el orden de las columnas (textrank_sentences prefiere las columnas en un orden determinado).


```{r}
article_sentences <- tibble(text = article) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)
```

a continuación haremos un token de nuevo, pero esta vez para conseguir palabras. Al hacer esto, mantendremos la columna sentence_id en nuestros datos.

```{r}
article_words <- article_sentences %>%
  unnest_tokens(word, sentence)
article_words
```
```{r}
article_words %>%
  count(word, sort = TRUE) 
```


```{r}

library(ggplot2)

article_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 6) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

ahora tenemos todas las entradas suficientes para la función textrank_sentences. Sin embargo, iremos un paso más allá y eliminaremos las palabras de stop en article_words ya que aparecerían en la mayoría de las frases y realmente no contienen ninguna información en sí mismas.

```{r}
article_words <- article_words %>%
  anti_join(data_frame(word = stopwords(kind = "es")), by = "word")

```

Si volvemos a visualizar el gráfico anterior podemos observar como hemos eliminado las palabras que no aportaban valor a las frases, tales como los artículos y conjunciones.

```{r}
article_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 4) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
Ejecutamos el algoritmo TextRank sólo requiere 2 entradas.

*Un marco de datos con frases
*Un data.frame con tokens (en nuestro caso palabras) que forman parte de cada frase.
Así que estamos listos para correr


```{r}
article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)
article_summary
```

Si bien el método de impresión es bueno, podemos extraer la información para un buen análisis posterior. La información sobre las frases se almacena en frases. Incluye la información article_sentences más la puntuación de textrank calculada. Si miramos el artículo a lo largo del tiempo, sería interesante ver dónde aparecen las frases importantes. En el gráfico siguiente podemos observa que frases poseenmayor score.

```{r}
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score")
```

